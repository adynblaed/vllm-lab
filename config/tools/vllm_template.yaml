# ██╗   ██╗██╗     ██╗     ███╗   ███╗    ███████╗███████╗██████╗ ██╗   ██╗███████╗██████╗ 
# ██║   ██║██║     ██║     ████╗ ████║    ██╔════╝██╔════╝██╔══██╗██║   ██║██╔════╝██╔══██╗
# ██║   ██║██║     ██║     ██╔████╔██║    ███████╗█████╗  ██████╔╝██║   ██║█████╗  ██████╔╝
# ╚██╗ ██╔╝██║     ██║     ██║╚██╔╝██║    ╚════██║██╔══╝  ██╔══██╗╚██╗ ██╔╝██╔══╝  ██╔══██╗
#  ╚████╔╝ ███████╗███████╗██║ ╚═╝ ██║    ███████║███████╗██║  ██║ ╚████╔╝ ███████╗██║  ██║
#   ╚═══╝  ╚══════╝╚══════╝╚═╝     ╚═╝    ╚══════╝╚══════╝╚═╝  ╚═╝  ╚═══╝  ╚══════╝╚═╝  ╚═╝
#
# Docs: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html

vllm:
  # Server Settings
  server:
    host: 0.0.0.0                            # Bind address for the server
    port: 8000                               # Port number for the vLLM server
    uvicorn_log_level: info                  # Logging level for Uvicorn (debug/info/warning/error/critical/trace)
    # allow_credentials: false                 # Enable CORS credentials
    # allowed_origins: ["*"]                   # CORS allowed origins
    # allowed_methods: ["GET", "POST", "OPTIONS"] # CORS allowed HTTP methods
    # allowed_headers: ["*"]                   # CORS allowed headers
    # api_key: ${VLLM_API_KEY}                 # API key for authentication (from env var)
    # lora_modules: []                         # LoRA module configs (name=path format)
    # prompt_adapters: []                      # Prompt adapter configs (name=path format)
    # chat_template: null                      # Custom chat template file path or inline string
    # response_role: assistant                 # Default role for chat completion responses
    # root_path: null                          # FastAPI root path for proxied setups
    # middleware: []                           # Additional ASGI middleware (import paths)
    # return_tokens_as_token_ids: false        # Return token IDs instead of strings for max_logprobs
    # disable_frontend_multiprocessing: false  # Run OpenAI frontend in same process as model engine

  # Model Configuration
  model:
    name: neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8 # HuggingFace model name or local path
    download_dir: /data                      # Directory to download and store model files
    load_format: auto                        # Model loading format (auto/pt/safetensors/npcache/dummy/tensorizer/bitsandbytes)
    dtype: auto                              # Data type for model weights and activations    
    tokenizer: null                          # Custom tokenizer name/path (if different from model)
    tokenizer_mode: auto                     # Tokenizer mode (auto/slow)
    max_model_len: null                      # Override model's maximum context length, should match max_seq_len_to_capture
    kv_cache_dtype: auto                     # Data type for KV cache
    skip_tokenizer_init: false               # Skip tokenizer initialization    
    trust_remote_code: false                 # Trust remote code from HuggingFace   
    # revision: null                           # Specific model version (branch/tag/commit ID)    
    # code_revision: null                      # Specific revision for model code on Hugging Face Hub
    # tokenizer_revision: null                 # Specific tokenizer version

  # Advanced Model Settings
  advanced:
    # quantization: null                       # Method for weight quantization
    # quantization_param_path: null            # Path to JSON file with KV cache scaling factors    
    # enable_lora: false                       # Enable LoRA adapter handling    
    # rope_theta: null                         # RoPE theta parameter
    # tokenizer_pool_size: 0                   # Size of tokenizer pool for async tokenization
    # tokenizer_pool_type: ray                 # Type of tokenizer pool (ray)
    # tokenizer_pool_extra_config: null        # Extra config for tokenizer pool (JSON string)
    # max_loras: 1                             # Maximum number of LoRAs in a single batch
    # max_lora_rank: 16                        # Maximum LoRA rank
    # lora_extra_vocab_size: 256               # Maximum size of extra vocabulary in LoRA adapters
    # lora_dtype: auto                         # Data type for LoRA
    # long_lora_scaling_factors: null          # Scaling factors for multiple LoRA adapters
    # max_cpu_loras: null                      # Maximum number of LoRAs to store in CPU memory
    # fully_sharded_loras: false               # Fully shard LoRA computation
    # enable_prompt_adapter: false             # Enable prompt adapter handling
    # max_prompt_adapters: 1                   # Maximum number of prompt adapters in a batch
    # max_prompt_adapter_token: 0              # Maximum number of prompt adapter tokens
    # scheduler_delay_factor: 0.0              # Delay factor for prompt scheduling
    # guided_decoding_backend: outlines        # Backend for guided decoding (outlines/lm-format-enforcer)
    # num_lookahead_slots: 0                   # Slots for speculative decoding (experimental)
    # max_logprobs: 20                         # Maximum number of log probabilities to return
    # rope_scaling: null                       # RoPE scaling configuration (JSON format)

# Performance Settings (GPU and Compute-related)
  performance:
    device: auto                             # Device type for vLLM execution
    max_seq_len_to_capture: null             # Maximum sequence length for CUDA graph capture    
    gpu_memory_utilization: 0.9              # Fraction of GPU memory to use (0.0 to 1.0)
    max_num_seqs: 256                        # Maximum number of sequences per iteration    
    distributed_executor_backend: mp         # Backend for distributed serving (ray/mp)
    swap_space: 16                           # CPU swap space size (GiB) per GPU
    tensor_parallel_size: 1                  # Number of tensor parallel replicas       
    # pipeline_parallel_size: 1                # Number of pipeline parallel stages
    # max_parallel_loading_workers: null       # Number of workers for parallel model loading
    # block_size: 16                           # Token block size for contiguous processing
    # enable_prefix_caching: false             # Enable automatic prefix caching
    # enforce_eager: false                     # Always use eager-mode PyTorch
    # cpu_offload_gb: 0                        # Space (GiB) to offload to CPU per GPU
    # disable_sliding_window: false            # Disable sliding window attention
    # use_v2_block_manager: false              # Use BlockSpaceManagerV2
    # num_gpu_blocks_override: null            # Override number of GPU blocks (for testing)
    # max_num_batched_tokens: null             # Maximum number of batched tokens per iteration
    # disable_custom_all_reduce: false         # Disable custom all-reduce implementation
    # enable_chunked_prefill: false            # Enable chunked prefill for large batches
    # speculative_model: null                  # Name of the draft model for speculative decoding
    # num_speculative_tokens: null             # Number of speculative tokens to sample
    # speculative_draft_tensor_parallel_size: null # Tensor parallel replicas for draft model
    # speculative_max_model_len: null          # Maximum sequence length for draft model
    # speculative_disable_by_batch_size: null  # Disable speculative decoding based on batch size
    # ngram_prompt_lookup_max: null            # Max window size for ngram prompt lookup
    # ngram_prompt_lookup_min: null            # Min window size for ngram prompt lookup
    # spec_decoding_acceptance_method: rejection_sampler # Acceptance method for speculative decoding
    # typical_acceptance_sampler_posterior_threshold: null # Posterior threshold for typical acceptance sampler
    # typical_acceptance_sampler_posterior_alpha: null # Alpha for typical acceptance sampler
    # disable_logprobs_during_spec_decoding: null # Disable log probabilities during speculative decoding
    # ray_workers_use_nsight: false            # Use Nsight to profile Ray workers

  # System and Logging Configuration
  system:
    seed: 0                                  # Random config seed for reproducibility
    disable_log_stats: false                 # Disable logging of statistics
    model_loader_extra_config: null          # Extra config for model loader (JSON string)
    ignore_patterns: []                      # Patterns to ignore when loading the model
    preemption_mode: null                    # Preemption mode (recompute/swap)
    served_model_name: []                    # Model name(s) used in the API
    qlora_adapter_name_or_path: null         # Name or path of the QLoRA adapter
    otlp_traces_endpoint: null               # Target URL for OpenTelemetry traces
    engine_use_ray: false                    # Use Ray to start LLM engine in separate process
    disable_log_requests: false              # Disable logging of requests
    max_log_len: null                        # Maximum length for logged prompts

  # Launcher Settings
  launcher:
    log_file: logs/vllm_server.log             # Main log file path
    health_check_directory: logs/health_checks # Directory for health check logs
    log_format: json                           # Log format (json for structured logging)
    log_level: INFO                            # Log level (INFO, DEBUG, etc.)
    log_performance_metrics: true              # Enable logging of performance metrics
    log_rotation_backup_count: 72              # Number of rotated log files to keep
    log_rotation_interval: 1                   # Log rotation interval (hours)
    log_rotation_max_bytes: 104857600          # Maximum log file size before rotation
    performance_log_interval: 30               # Interval for logging performance metrics (seconds)

    resource_limits:
      # max_concurrent_requests: 100             # Maximum number of concurrent requests
      # request_timeout_seconds: 300             # Request timeout in seconds
      # max_batch_size: 32                       # Maximum batch size for inference

    # Security Configuration
    security:
      # ssl_keyfile: ${SSL_KEYFILE_PATH}           # Path to SSL key file
      # ssl_certfile: ${SSL_CERTFILE_PATH}         # Path to SSL certificate file
      # ssl_ca_certs: null                         # Path to CA certificates file
      # ssl_cert_reqs: 0                           # Client certificate requirements
  
    # Logging Configuration
    logging:
      correlation_id_header: X-Correlation-ID    # Header for request correlation ID
      log_request_details: true                  # Log details of incoming requests
      max_log_len: 1000                          # Maximum length for log messages
      sanitize_log_data: true                    # Remove sensitive data from logs
      
      health_check:                              # Settings for completions in test/vllm_server_health_check.py
        prompts:
          system_message: "You are an AI Assistant that responds to 'INFERENCE ACKNOWLEDGEMENT REQUESTS:' on route: /v1/chat/completions"
          user_message: "INFERENCE ACKNOWLEDGEMENT REQUEST: Is this endpoint route online and available for requests? Please respond with your acknowledgement in haiku."
